na_count <- sapply(ob2NA, function(y) sum(is.na(y)))


ob_ <- ob2[,na_count !=0]

datset <- rbind(traindatNA[,-160],testdatNA[,-160])
> datpre <- preProcess(datset, method = c("nzv","zv","corr"))
> datset1 <- predict(datpre,datset)
> anyNA(datset1)
na_count <- sapply(datset, function(y) sum(is.na(y)))
datset1 <- datset[,(na_count/nrow(datset)) < 0.75]
sans_test <- datset1[-c(1623:1642),]
anyNA(sans_test)

sans_test$classe <- traindatNA[,160]
> datpre__ <- preProcess(sans_test, method = c("nzv","zv","corr"))
> datpre__ <- preProcess(sans_test[,-160], method = c("nzv","zv","corr"))
> pred_sans <- predict(datpre__,sans_test)

> pred_sans <- pred_sans[,-3:6]
finaltest <- datset1[19623:19642,]

model <- train(classe ~ ., data = pred_sans, method= "lda")

train_resultado <- predict(model,pred_sans)
confusionMatrix(train_resultado,pred_sans[,"classe"])

model_rpart <- train(classe ~ ., data = pred_sans, method= "rpart")
confusionMatrix(predict(model_rpart,pred_sans),pred_sans[,"classe"])

#run rf and nb models


------Friday Oct 13th -------

datset2 <- datset[,(na_count/nrow(datset)) < 0.9]
> anyNA(datset2)

> sans_test2 <- datset2[-c(1623:1642),]
> anyNA(datset2)

> tail(colnames(sans_test2))

> sans_test2$classe <- traindatNA[,160]
> nzv <- nearZeroVar(sans_test2)
> head(nzv)


> sans_test2 <- sans_test2[,-nzv]
> sans_test2 <- predict(preProcess(sans_test2[-59], method = "corr"),sans_test2)

  
> sans_test2 <- sans_test2[,-c(1:5)]
> ldamod <- train(classe ~ ., data = sans_test2, method="lda")
> confusionMatrix(predict(ldamod,sans_test2),sans_test2[,"classe"])

> finaltest2 <- datset2[19623:19642,]
> View(datset2)
> View(finaltest2)
> View(testdatNA)
> testdatNA[,"problem_id"]

predict(ldamod,finaltest2)

 rpartmod <- train(classe ~ ., data = sans_test2, method="rpart")
 
 
  - so far lda is better than rpart; but it's still not enough to pass
  - Try combining them or boosting
  
  
  2  A
  3  B
  4  A
  5  A
  7  D
  9  A
  10 A
  14 A
  15 E
  17 A
  18 B
  19 B
  20 B
  
   The goal of this assignment is to predict the manner in which an exercise was performed. For that we need to make sure that the training and test sets have some common rules, i.e., perform some data cleaning.
 
   Our first task will be to join the training and test and remove columns with mostly zero-values and near-zero values. Then we will remove all columns where 90% of their values are NAs. 
   
   Now we have a data set w/out columns composed of near-zero values, zero-values, and NA, which makes us ready to split the dataframe back into training and a test dataset.
   
   On the train df, it is imperative to do some variable selection in order to get rid of some of the noise of variables that have nothing to do with the outcome. Some of these variables are: X, user_name, the timestamps(3).
   
   We consider our training set clean enough for a fair shot at modeling and accurate predictions.
   Four types of model will be created and applied on the train set and then on the test set: lda, rpart, a combination of the previous two, and radomForrest.
   
  (lda modeling, prediction on train data and then test data)
  (rpart modeling, prediction on train data and then test data)
  
  So far we can see that lda performs better than rpart, however it still isn't accurate enough. We'll try combininig them to see if it gets more accurate.
  
  (gam_of_lda_rpart modeling, prediction on train data and then test data)
  
  As we can see, even combining lda and rpart is not enough for a good prediction. We'll now try randomForrests (it is the last to be tried due to it being more computationally demanding)
  
  we will apply a couple of tweaks to randomForrests in order to make it less computationally demanding.(EXPLAIN TWEAKS LIVE CV ON RANDOMFORREST)
  
  (model and predict using rf)
  (explain results)
  (make a statement about the expected out of sample error)

  FRIDAY NOV 10TH
  
  ---
title: "Exercise Manner Prediction"
author: "Hugo Barros"
date: "November 10, 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

 The goal of this assignment is to predict the manner in which an exercise was performed.
 
 For that, we need to make sure that the training and test sets have some common rules, i.e., perform some data cleaning.
 
   We have a training and a test dataset. So our first task will be to read the two datasets(treating empty cells as NAs), and join them, without the last column of each (because they are different and also because they're not meant to go through data cleaning) into one dataframe.

```{r cars}

suppressPackageStartupMessages(library(randomForest))
suppressPackageStartupMessages(library(doParallel))
suppressPackageStartupMessages(library(caret))

traindat <- read.csv("c:/Users/Dafh/Documents/R/Work/Machine Learning/trainDat.csv", na.strings = c("",NA))

testdat <- read.csv("c:/Users/Dafh/Documents/R/Work/Machine Learning/testDat.csv", na.strings = c("",NA))

joint <- rbind(traindat[,-160], testdat[,-160])

```

   
   
   Now we will remove columns with only zero-values, near-zero values those where 90% or more of their values are NAs. 
   
```{r}
pre_joint <- preProcess(joint,method = c("zv","corr"))
joint1 <- predict(pre_joint,joint)
anyNA(joint1)


na_count <- sapply(joint1, function(y) sum(is.na(y)))

zeroNA <- joint1[,(na_count/nrow(joint1)) < 0.90]

```
   Now we have a dataframe w/out columns composed of only near-zero values,or zero-values, or NAs, which makes us ready to split the dataframe back into a training and a test datasets.
   We will also add the last column of training data(classe) that we removed when we joined the two datasets
   
```{r}
trClean <- zeroNA[1:19622,]
testClean <- zeroNA[19623:19642,]
trClean$classe <- traindat$classe

```
  
   On the train dataframe, it is imperative to do some variable selection in order to get rid of some of the noise of variables that are not likely to have an impact on the outcome. Some of these variables are: X, user_name, the timestamps(all three of them).
   
```{r}   
trClean <- trClean[,-c(1:6)]
```
   
   We consider our training set clean enough for a fair shot at modeling and accurate predictions.
   
   Four types of model will be created and applied on the train set and then on the test set: lda, rpart, a combination of the previous two, and radomForrest.
   
  (lda modeling, prediction on train data and then test data)
```{r }  
 tail(colnames(trClean)) 
#head(trClean[,"classe"])
ldamod <- train(classe ~ ., data = trClean, method="lda")
confusionMatrix(predict(ldamod,trClean),trClean[,46])$overall[1]
```
 
 
 
(rpart modeling, prediction on train data and then test data)
```{r }
rpartmod <- train(classe ~ ., data = trClean, method="rpart")
confusionMatrix(predict(rpartmod,trClean),trClean[,"classe"])$overall[1]

```

   As far as accuracy on the train data, lda is more accurate than rpart.Still, both models are not very accurate even on the data where it was trained. However, that does not mean it won't perform better on the test data.

  
  We will now try both models on the test data.
  
  Let's now try it on the test data:
  
  LDA:
```{r}
  lda_test_prediction <- predict(ldamod,testClean)
  lda_test_prediction
```
RPART:
```{r}
  lda_test_prediction <- predict(ldamod,testClean)
  lda_test_prediction
```


  So far we can see that lda performs better than rpart, however it still isn't accurate enough. We'll try combininig them to see if it gets more accurate.
  
  (gam_of_lda_rpart modeling, prediction on train data and then test data)
  
  As we can see, even combining lda and rpart is not enough for a good prediction. We'll now try randomForrests (it is the last to be tried due to it being more computationally demanding)
  
  we will apply a couple of tweaks to randomForrests in order to make it less computationally demanding.(EXPLAIN TWEAKS LIVE CV ON RANDOMFORREST)
  
  (model and predict using rf)
  (explain results)
  (make a statement about the expected out of sample error)

--------------------------MORE BELOW--------------------------

my_cluster <- makeCluster(detectCores()-1)

registerDoParallel(my_cluster)
fitControl <- trainControl(method = "cv", number = 10, allowParallel = TRUE)
x <- trClean[,-46]
y <- trClean[,46]
fit <- train(x,y,method = "rf", data=trClean,trControl = fitControl)
predict(fit,testClean)

----------------------------------------------------------------------------------------------------------



DATA PRODUCTS WEEK 2

---
title: "Luanda/Angola - Culture/Food/fun"
author: "Hugo Barros"
date: "November 10, 2017"
output: html_document
---

```{r setup, include=FALSE}
library(googleVis)
library(leaflet)
library(dplyr)
```




```{r echo=FALSE, warning=FALSE, message=FALSE}

luandaLatLong <- data.frame( longitude = c(13.238299, 13.223020, 13.227774, 13.233390, 13.254104, 13.258368, 13.225311, 13.235025,
                                     13.222916, 13.230295, 13.234420, 13.225522),
                             latitude = c(-8.815075, -8.808102, -8.813558, -8.810624, -8.768558, -8.765717, -8.796232, -8.814222,
                                     -8.799479, -8.814573, -8.815032, -8.795400 ),
                              col = c( rep("green",4), rep("red",4), rep("blue",4)), stringsAsFactors = FALSE)

siteNames <- c("Museu de Historia Natural", "Fortaleza S. Miguel", "Museu de Antropologia", "Museu da Moeda", "Chillout","Players","Bay Inn","Switch",
               "Jango Veleiro", "Kitanda da Esquina", "Asia Lounge", "Cais de 4")


luandaLatLong %>% leaflet() %>% addTiles() %>% addCircleMarkers(color=luandaLatLong$col,  popup=siteNames, label = siteNames) %>% 
        addLegend(labels = c("Museum", "Club", "Restaurant"), colors = c("green","red","blue"))

```


