na_count <- sapply(ob2NA, function(y) sum(is.na(y)))


ob_ <- ob2[,na_count !=0]

datset <- rbind(traindatNA[,-160],testdatNA[,-160])
> datpre <- preProcess(datset, method = c("nzv","zv","corr"))
> datset1 <- predict(datpre,datset)
> anyNA(datset1)
na_count <- sapply(datset, function(y) sum(is.na(y)))
datset1 <- datset[,(na_count/nrow(datset)) < 0.75]
sans_test <- datset1[-c(1623:1642),]
anyNA(sans_test)

sans_test$classe <- traindatNA[,160]
> datpre__ <- preProcess(sans_test, method = c("nzv","zv","corr"))
> datpre__ <- preProcess(sans_test[,-160], method = c("nzv","zv","corr"))
> pred_sans <- predict(datpre__,sans_test)

> pred_sans <- pred_sans[,-3:6]
finaltest <- datset1[19623:19642,]

model <- train(classe ~ ., data = pred_sans, method= "lda")

train_resultado <- predict(model,pred_sans)
confusionMatrix(train_resultado,pred_sans[,"classe"])

model_rpart <- train(classe ~ ., data = pred_sans, method= "rpart")
confusionMatrix(predict(model_rpart,pred_sans),pred_sans[,"classe"])

#run rf and nb models


------Friday Oct 13th -------

datset2 <- datset[,(na_count/nrow(datset)) < 0.9]
> anyNA(datset2)

> sans_test2 <- datset2[-c(1623:1642),]
> anyNA(datset2)

> tail(colnames(sans_test2))

> sans_test2$classe <- traindatNA[,160]
> nzv <- nearZeroVar(sans_test2)
> head(nzv)


> sans_test2 <- sans_test2[,-nzv]
> sans_test2 <- predict(preProcess(sans_test2[-59], method = "corr"),sans_test2)

  
> sans_test2 <- sans_test2[,-c(1:5)]
> ldamod <- train(classe ~ ., data = sans_test2, method="lda")
> confusionMatrix(predict(ldamod,sans_test2),sans_test2[,"classe"])

> finaltest2 <- datset2[19623:19642,]
> View(datset2)
> View(finaltest2)
> View(testdatNA)
> testdatNA[,"problem_id"]

predict(ldamod,finaltest2)

 rpartmod <- train(classe ~ ., data = sans_test2, method="rpart")
 
 
  - so far lda is better than rpart; but it's still not enough to pass
  - Try combining them or boosting
  
  
  2  A
  3  B
  4  A
  5  A
  7  D
  9  A
  10 A
  14 A
  15 E
  17 A
  18 B
  19 B
  20 B
  
   The goal of this assignment is to predict the manner in which an exercise was performed. For that we need to make sure that the training and test sets have some common rules, i.e., perform some data cleaning.
 
   Our first task will be to join the training and test and remove columns with mostly zero-values and near-zero values. Then we will remove all columns where 90% of their values are NAs. 
   
   Now we have a data set w/out columns composed of near-zero values, zero-values, and NA, which makes us ready to split the dataframe back into training and a test dataset.
   
   On the train df, it is imperative to do some variable selection in order to get rid of some of the noise of variables that have nothing to do with the outcome. Some of these variables are: X, user_name, the timestamps(3).
   
   We consider our training set clean enough for a fair shot at modeling and accurate predictions.
   Four types of model will be created and applied on the train set and then on the test set: lda, rpart, a combination of the previous two, and radomForrest.
   
  (lda modeling, prediction on train data and then test data)
  (rpart modeling, prediction on train data and then test data)
  
  So far we can see that lda performs better than rpart, however it still isn't accurate enough. We'll try combininig them to see if it gets more accurate.
  
  (gam_of_lda_rpart modeling, prediction on train data and then test data)
  
  As we can see, even combining lda and rpart is not enough for a good prediction. We'll now try randomForrests (it is the last to be tried due to it being more computationally demanding)
  
  we will apply a couple of tweaks to randomForrests in order to make it less computationally demanding.(EXPLAIN TWEAKS LIVE CV ON RANDOMFORREST)
  
  (model and predict using rf)
  (explain results)
  (make a statement about the expected out of sample error)
